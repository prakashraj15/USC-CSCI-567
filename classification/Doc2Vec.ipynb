{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9c28a-c08e-4aea-b243-a6e964b4222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "import json\n",
    "from random import shuffle, randint\n",
    "from datetime import datetime\n",
    "from collections import namedtuple, OrderedDict\n",
    "import multiprocessing\n",
    "from smart_open import open\n",
    "from tqdm.auto import tqdm\n",
    "import gensim\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import time\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba50e4-a812-4808-b18a-a5cb67c7216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(X_train,y_train,X_test,y_test,svc_c=0,num_epochs=100,alpha=0.015):\n",
    "  stopwords_path='../utils/stopwords.txt'\n",
    "  vocab_min_count=5\n",
    "  vector_size=200\n",
    "  min_alpha=0.001\n",
    "  window=5\n",
    "  negative = 5\n",
    "  hs = 0\n",
    "  def read_lines(path):\n",
    "    return [line.strip() for line in codecs.open(path, \"r\", \"utf-8\")]\n",
    "  def load_stopwords(stopwords_path):\n",
    "    stopwords = read_lines(stopwords_path)\n",
    "    return dict(map(lambda w: (w.lower(), ''), stopwords))\n",
    "  assert gensim.models.doc2vec.FAST_VERSION > - \\\n",
    "        1, \"This will be painfully slow otherwise\"\n",
    "  stopwords = load_stopwords(stopwords_path)\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  docs=[]\n",
    "  for i , doc in enumerate(X_train):\n",
    "    words = doc.replace(\"\\n\",\" \").replace(\"ред\", \" \")\n",
    "    words = re.sub(r'[^\\w\\s]', \" \", words).split()\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 1]\n",
    "    tags=[i]\n",
    "    docs.append(TaggedDocument(words=words, tags=tags))\n",
    "    model = Doc2Vec(dm=1, dm_mean=1, vector_size=vector_size, window=window, negative=negative, hs=hs,\n",
    "                    min_count=vocab_min_count, workers=cores)\n",
    "  vocab_size = len(model.wv.index_to_key)\n",
    "  model.build_vocab(docs)\n",
    "  shuffle(docs)\n",
    "  print(\"Training\")\n",
    "  model.train(docs, total_examples=len(docs),\n",
    "              epochs=num_epochs, start_alpha=alpha, end_alpha=min_alpha,report_delay=60)\n",
    "  Xtr=[]\n",
    "  for i , doc in enumerate(X_train):\n",
    "    Xtr.append(model.dv.get_vector(i))\n",
    "  Xte=[]\n",
    "  for i , doc in enumerate(X_test):\n",
    "    words = doc.replace(\"\\n\",\" \").replace(\"ред\", \" \")\n",
    "    words = re.sub(r'[^\\w\\s]', \" \", words).split()\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 1]\n",
    "    Xte.append(model.infer_vector(words))\n",
    "  print(\"Classifying\")\n",
    "  clf = SVC()\n",
    "  clf.fit(Xtr, y_train)\n",
    "  y_pred = clf.predict(Xte)\n",
    "  print(\"SVC Accuracy: \",classification_report(y_test, y_pred,output_dict=True)['accuracy'])\n",
    "\n",
    "  clf = GradientBoostingClassifier()\n",
    "  clf.fit(Xtr, y_train)\n",
    "  y_pred = clf.predict(Xte)\n",
    "  print(\"GradientBoostingClassifier Accuracy: \",classification_report(y_test, y_pred,output_dict=True)['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/data.csv\"\n",
    "df=pd.read_csv(input_path)\n",
    "df['sentences']=df['sentences'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb565e61-83c7-4638-8ded-b6180584d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['sentences'].apply(lambda x: \" \".join(x))\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9f0bf-199d-49ca-b24b-69a7fd33e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf806e8-03d3-4027-8ab0-10b502e8659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec(X_train,y_train,X_test,y_test,svc_c=None,num_epochs=100,alpha=0.015)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLDC",
   "language": "python",
   "name": "hldc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
