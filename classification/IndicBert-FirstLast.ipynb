{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoConfig, AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data=pd.read_csv(path)\n",
    "\n",
    "    train=data.iloc[:9000]\n",
    "    test=data.iloc[9000:9500]\n",
    "    val=data.iloc[9500:10000]\n",
    "     \n",
    "    train['sentences']=train['sentences'].apply(eval)\n",
    "    test['sentences']=test['sentences'].apply(eval)\n",
    "    val['sentences']=val['sentences'].apply(eval)\n",
    "    return train, val, test\n",
    "\n",
    "input_path='../data/data.csv'\n",
    "train, val, test=get_data(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric1 = load_metric(\"accuracy\")\n",
    "metric2 = load_metric(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric1.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric2.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def compute_metrics_hyperparameter(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = metric1.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric2.compute(predictions=predictions, references=labels)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseDataset(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Create empty lists to store outputs\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "\n",
    "        encoded_sent = self.tokenizer.encode_plus(\n",
    "            text=self.data.iloc[idx]['sentences'][0],   \n",
    "            add_special_tokens=True,\n",
    "            max_length=512,                   \n",
    "            padding='max_length',             \n",
    "            return_attention_mask=True,       \n",
    "            truncation=True\n",
    "            )\n",
    "        \n",
    "        input_ids = encoded_sent.get('input_ids')\n",
    "        attention_mask = encoded_sent.get('attention_mask')\n",
    "        \n",
    "        ## Take the first 512 tokens \n",
    "        input_ids = input_ids[:512]\n",
    "        attention_mask = attention_mask[:512]\n",
    "        \n",
    "        ## Take the last 512 tokens\n",
    "        # input_ids = input_ids[-512:]\n",
    "        # attention_mask = attention_mask[-512:]\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)        \n",
    "        label = torch.tensor(self.data.iloc[idx]['label'])\n",
    "        # Create torch tensor with 1 value for each input\n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}\n",
    "train_dataset = CaseDataset(train, tokenizer)\n",
    "validation_dataset = CaseDataset(val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../results/classification/bert/first-last'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    num_train_epochs=100,         \n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,  \n",
    "    warmup_steps=50,              \n",
    "    weight_decay=0.01,             \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    learning_rate = 1e-5,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer( model=model,args=training_args,train_dataset=train_dataset,eval_dataset=validation_dataset,compute_metrics=compute_metrics)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CaseDataset(test, tokenizer)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "print(predictions.metrics['test_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLDC",
   "language": "python",
   "name": "hldc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
