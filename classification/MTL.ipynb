{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe981ad-d6f6-4dba-a7ef-816e142ba583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AdamW\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52261758-7a9f-4077-96a3-55edc19a6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data=pd.read_csv(path)\n",
    "\n",
    "    train=data.iloc[:9000]\n",
    "    test=data.iloc[9000:9500]\n",
    "    val=data.iloc[9500:10000]\n",
    "\n",
    "    train['ranked-sentences']=train['ranked-sentences'].apply(eval)\n",
    "    test['ranked-sentences']= test['ranked-sentences'].apply(eval)\n",
    "    val['ranked-sentences']=val['ranked-sentences'].apply(eval)\n",
    "    train['sentences']=train['sentences'].apply(eval)\n",
    "    test['sentences']=test['sentences'].apply(eval)\n",
    "    val['sentences']=val['sentences'].apply(eval)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.labels = self.df['label']\n",
    "        self.ranked_sentences = self.df['ranked-sentences']\n",
    "        self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        lines = self.df.iloc[idx]['sentences']\n",
    "        embeddings = self.sentence_model.encode(\n",
    "            lines\n",
    "        )\n",
    "\n",
    "        salience_labels = [0] * len(lines)\n",
    "        indices = [lines.index(i) for i in self.ranked_sentences.iloc[idx]]\n",
    "        for i in indices[:len(salience_labels)//2]:\n",
    "            salience_labels[i] = 1\n",
    "\n",
    "\n",
    "        sample['embeddings'] = torch.from_numpy(embeddings)\n",
    "        sample['label'] = torch.Tensor([self.df.iloc[idx]['label']])\n",
    "        sample['salience_labels'] = torch.LongTensor(salience_labels)\n",
    "\n",
    "        return sample \n",
    "    \n",
    "def custom_collate(batch):\n",
    "    labels, salience_labels, embs = [], [], []\n",
    "    for item in batch:\n",
    "        labels.append(item['label'])\n",
    "        salience_labels.append(item['salience_labels'])\n",
    "        embs.append(item['embeddings'])\n",
    "\n",
    "    labels = pad_sequence(labels, batch_first=True)\n",
    "    embs = pad_sequence(embs, batch_first=True)\n",
    "    salience_labels = pad_sequence(salience_labels, padding_value=-100, batch_first=True)\n",
    "    return embs, labels.long(), salience_labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08db25-6d12-46cc-809e-1f1d2d4fc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nhead=1,\n",
    "                 nlayers=1,\n",
    "                 use_cls=True,\n",
    "                 #  cls_embed=None,\n",
    "                 d_model=768):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.saliency_classifier = nn.Linear(d_model, 2)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Use [cls] token or pooling output for bail prediction\n",
    "        self.use_cls = use_cls\n",
    "        self.d_model = d_model\n",
    "        if use_cls:\n",
    "            self.cls_embed = nn.Embedding(1, self.d_model)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            batch_first=True),\n",
    "            nlayers,\n",
    "            norm=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        if self.use_cls:\n",
    "            x = torch.cat([self.cls_embed.weight[0].unsqueeze(\n",
    "                0).repeat(batch_size, 1, 1), x], dim=1)\n",
    "\n",
    "        x = self.encoder_layer(x)\n",
    "\n",
    "        if self.use_cls:\n",
    "            label_x = x[:, 0, :]\n",
    "            saliency_x = x[:, 1:, :]\n",
    "        else:\n",
    "            label_x = torch.sum(x, dim=1)\n",
    "            saliency_x = x\n",
    "        label_logits = self.classifier(label_x)  \n",
    "        saliency_logits = self.saliency_classifier(\n",
    "            saliency_x)  \n",
    "        return label_logits, saliency_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995dd80-35c7-4b1a-b78b-583d6a56e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step( model, dataloader, device, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        embeddings, label, saliency_label = batch \n",
    "        embeddings = embeddings.to(device)\n",
    "        label = label.to(device)\n",
    "        saliency_label = saliency_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        label_logits, saliency_logits = model(embeddings)  \n",
    "\n",
    "        saliency_logits = saliency_logits.contiguous().view(-1, saliency_logits.size(-1))\n",
    "        saliency_label = saliency_label.contiguous().view(-1)\n",
    "        loss_label = F.cross_entropy(label_logits, label.squeeze(1))\n",
    "        loss_saliency = F.cross_entropy(saliency_logits, saliency_label)\n",
    "\n",
    "        loss = loss_label + loss_saliency \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a36ca8-ee78-4220-a2e1-880340fe360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "            embeddings, label, saliency_label = batch \n",
    "            embeddings = embeddings.to(device)\n",
    "            label = label.to(device)\n",
    "            saliency_label = saliency_label.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                label_logits, saliency_logits = model(embeddings) ## mask\n",
    "\n",
    "            saliency_logits = saliency_logits.contiguous().view(-1, saliency_logits.size(-1))\n",
    "            saliency_label = saliency_label.contiguous().view(-1)\n",
    "\n",
    "            loss_label = F.cross_entropy(label_logits, label.squeeze(1))\n",
    "            loss_saliency = F.cross_entropy(saliency_logits, saliency_label)\n",
    "\n",
    "            loss = loss_label + loss_saliency \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_label = torch.argmax(label_logits, dim=1).flatten().cpu().numpy()\n",
    "            predictions.append(pred_label)\n",
    "            targets.append(label.squeeze(1).cpu().numpy())\n",
    "\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    epoch_loss = total_loss/len(dataloader)\n",
    "    return epoch_loss, targets, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb07c4-9999-44c3-8d58-8150b35a29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../results/ranked/\"\n",
    "batch_size = 16\n",
    "files = os.listdir(\"../results/ranked/\")\n",
    "\n",
    "device =torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "train, val, test = get_data(input_path+files[0])\n",
    "train_dataset = Dataset(train)\n",
    "val_dataset = Dataset(val)\n",
    "test_dataset = Dataset(test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "save_model = True \n",
    "os.makedirs(\"../models/\", exist_ok=True)\n",
    "model_path = \"../models/multitask.pt\"\n",
    "d_model = 768\n",
    "\n",
    "model = MultiTaskModel(d_model=d_model)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5*1e-5)\n",
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_step(model, train_dataloader, device, optimizer)\n",
    "\n",
    "    val_loss,_,_ = eval_step(model, val_dataloader, device)\n",
    "\n",
    "    print(f\"\\nEpoch: {epoch+1} | Training loss: {train_loss} | Validation Loss: {val_loss}\")\n",
    "    if (val_loss < best_loss) and (save_model == True):\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_dict = torch.load(model_path,  map_location=device)\n",
    "model.load_state_dict(loaded_state_dict)\n",
    "\n",
    "test_loss,targets,predictions = eval_step(model, test_dataloader, device)\n",
    "accuracy = np.sum(targets == predictions)/len(targets)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "ConfusionMatrixDisplay.from_predictions(targets, predictions)\n",
    "# plt.savefig(f\"./confusion_matrix.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLDC",
   "language": "python",
   "name": "hldc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
