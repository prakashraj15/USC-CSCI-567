{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "import math\n",
    "from deep_translator import GoogleTranslator\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=open('../utils/hindi.txt')\n",
    "hindi_stop_words=[]\n",
    "for x in stop:\n",
    "    hindi_stop_words.append(x.strip(\"\\n\"))\n",
    "\n",
    "def tokenize_hindi(sentence):\n",
    "    tokens = indic_tokenize.trivial_tokenize(sentence, lang='hi')\n",
    "    \n",
    "    filtered_tokens = [token for token in tokens if token not in hindi_stop_words]\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTokens(df):\n",
    "    tokensCollection=[]\n",
    "    for i, row in df.iterrows():\n",
    "        tokens=[tokenize_hindi(sentence ) for sentence in row['sentences']]\n",
    "        tokensCollection.append(tokens)\n",
    "    df['tokens'] = tokensCollection\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_lexRank(sentences,tokens,threshold=0.1,epsilon=0.1):\n",
    "    freq_matrix = {}\n",
    "    ts=len(tokens)\n",
    "    for i,token in enumerate(tokens):\n",
    "        freq_table = {}\n",
    "        words = token\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        freq_matrix[i] = freq_table\n",
    "    tf_matrix = {}\n",
    "    c = 0\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[c] = tf_table\n",
    "        c = c+1\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "    idf_matrix = {}\n",
    "    idf_table = {}\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "\n",
    "        for word in f_table.keys():\n",
    "          if word not in idf_table:\n",
    "            idf_table[word] = math.log10(ts / float(word_per_doc_table[word]))\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "    matrix = np.zeros((ts, ts))\n",
    "    s = {}\n",
    "    for i in range(ts):\n",
    "      for j in range(ts):\n",
    "        u1 = tokens[i]\n",
    "        u2 = tokens[j]\n",
    "        common = list(set(u1) & set(u2))\n",
    "        d1=0\n",
    "        d2=0\n",
    "        n=0.0\n",
    "        for t in u1:\n",
    "          if t in tf_matrix[i]:\n",
    "            tf1=tf_matrix[i][t]\n",
    "          else:\n",
    "            tf1=0\n",
    "          if t in idf_table:\n",
    "            idf1=idf_table[t]\n",
    "          else:\n",
    "            idf1=0\n",
    "          d1+=(tf1*idf1)**2\n",
    "        for t in u2:\n",
    "          if t in tf_matrix[j]:\n",
    "            tf2=tf_matrix[j][t]\n",
    "          else:\n",
    "            tf2=0\n",
    "          if t in idf_table:\n",
    "            idf2=idf_table[t]\n",
    "          else:\n",
    "            idf2=0\n",
    "          d2+=(tf2*idf2)**2\n",
    "        for t in common:\n",
    "          if t in tf_matrix[i]:\n",
    "            tfc1=tf_matrix[i][t]\n",
    "          else:\n",
    "            tfc1=0\n",
    "          if t in tf_matrix[j]:\n",
    "            tfc2=tf_matrix[j][t]\n",
    "          else:\n",
    "            tfc2=0\n",
    "          if t in idf_table:\n",
    "            idf=idf_table[t]\n",
    "          else:\n",
    "            idf=0\n",
    "          n+=tfc1*tfc2*idf**2\n",
    "        if d1 > 0 and d2 > 0:\n",
    "          matrix[i][j] =  n / (math.sqrt(d1) * math.sqrt(d2))\n",
    "        else:\n",
    "          matrix[i][j] = 0.0\n",
    "    degrees = np.zeros((ts, ))\n",
    "    for i in range(ts):\n",
    "      for j in range(ts):\n",
    "        if matrix[i, j] > threshold:\n",
    "          matrix[i, j] = 1.0\n",
    "          degrees[i] += 1\n",
    "        else:\n",
    "          matrix[i, j] = 0\n",
    "    for i in range(ts):\n",
    "        for j in range(ts):\n",
    "            if degrees[i] == 0:\n",
    "                degrees[i] = 1\n",
    "\n",
    "            matrix[i][j] = matrix[i][j] / degrees[i]\n",
    "    transposed_matrix = matrix.T\n",
    "    p_vector = np.array([1.0 / ts] * ts)\n",
    "    lambda_val = 1.0\n",
    "\n",
    "    while lambda_val > epsilon:\n",
    "      next_p = np.dot(transposed_matrix, p_vector)\n",
    "      lambda_val = np.linalg.norm(np.subtract(next_p, p_vector))\n",
    "      p_vector = next_p\n",
    "    avg = np.sum(p_vector) / len(p_vector)\n",
    "    sentence_ids = []\n",
    "    for i in range(ts):\n",
    "      if(p_vector[i]>=avg):\n",
    "        sentence_ids.append(i)\n",
    "    summary = [sentences[i] for i in sentence_ids]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path=\"../data/\"\n",
    "output_path=\"../results/summary/\"\n",
    "if os.path.exists(output_path) == False:\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "data = pd.read_csv(f'{input_path}data.csv')\n",
    "data['sentences'] = data['sentences'].apply(eval)\n",
    "sentences = data['sentences']\n",
    "data = extractTokens(data)\n",
    "tokens = data['tokens'] \n",
    "summary=[]\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    summary.append(summarize_lexRank( sentences[i] ,tokens[i]))\n",
    "\n",
    "data['summary'] = summary\n",
    "\n",
    "data.to_csv(f'{output_path}lexrank.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLDC",
   "language": "python",
   "name": "hldc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
